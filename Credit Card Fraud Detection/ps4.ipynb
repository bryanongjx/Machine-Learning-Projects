{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 4: Credit Card Fraud Detection\n",
    "\n",
    "**Release Date:** 30 September 2022\n",
    "\n",
    "**Due Date:** 23:59, 22 October 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In class, we discussed about logistic regression, and how it can be useful as a classification algorithm. In this problem set, we get some hands-on practice by implementing logistic regression on a Credit Card Fraud Detection dataset. Note that for this problem set, you should only be using the scikit-learn (sklearn) library for the last part (Tasks 5.x) on SVM.\n",
    "\n",
    "**Required Files**:\n",
    "\n",
    "* ps4.ipynb\n",
    "* credit_card.csv\n",
    "* restaurant_data.csv\n",
    "\n",
    "**Honour Code**: Note that plagiarism will not be condoned! You may discuss with your classmates and check the internet for references, but you MUST NOT submit code/report that is copied directly from other sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The context is that there are fraudulent credit card transactions. Therefore, we hope to help credit card companies recognize those fraudulent transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "We are given a dataset that contains transactions made by credit cards holders in `credit_card.csv`. If we think about what type of data might be included in the input variables under the given context, we might realize that those input variables are likely to include word descriptions, such as shop name or locations. In this problem set, we don't need to worry about language processing as the data are pre-processed to contain only numeric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Take a look at the columns in the dataset `credit_card.csv`. We have V1-V20, 'Amount', and 'Time' as input features, and 'Class' as output which takes the value 1 if it's fraud and 0 otherwise. This dataset presents 492 frauds out of 284,807 transactions. That means, there are 284,808 rows (including the header) in the csv file.\n",
    "\n",
    "We will use this dataset to implement logistic regression using batch and stochastic gradient descent for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports and setup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "\n",
    "########################\n",
    "# Read credit card     #\n",
    "# data for large tests #\n",
    "########################\n",
    "\n",
    "dirname = os.getcwd()\n",
    "credit_card_data_filepath = os.path.join(dirname, 'credit_card.csv')\n",
    "restaurant_data_filepath = os.path.join(dirname, 'restaurant_data.csv')\n",
    "\n",
    "credit_df = pd.read_csv(credit_card_data_filepath)\n",
    "X = credit_df.values[:, :-1]\n",
    "y = credit_df.values[:, -1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Problem with imbalanced data\n",
    "\n",
    "Our first task is to describe in one or two sentences what problem we might encounter if we directly use the given dataset without processing it. (Hint: consider the 'Class' column, and think about how a high accuracy prediction can be achieved in a wrong way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Resampling methods\n",
    "\n",
    "When we are faced with the issue of imbalanced data, there are several ways to deal with it. A more direct way might be just to collect more data instances. We realized that in our case this doesn't work well because the events unevenly occur. We then look at how to resample the existing instances.\n",
    "\n",
    "In this problem set, you are introduced with three resampling methods: undersampling, oversampling, and SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 1.2.1: Undersampling\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/undersampling.png\" alt=\"visualisation of undersampling\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 1: Visualisation of undersampling.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates undersampling. In undersampling, we remove samples from the majority class. More specifically, we randomly take subsamples of the majority class such that the size of two classes is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.1: Random undersampling in practice\n",
    "\n",
    "Your task is to observe the data in credit\\_card.csv and randomly remove some observations of the majority class until the two classes balance out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_undersampling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given credit card dataset with 0 as the majority for 'Class' column. Returns credit card data with two classes\n",
    "    having the same shape, given raw data read from csv.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The potentially imbalanced dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Undersampled dataset with equal number of fraudulent and legitimate data.\n",
    "    '''\n",
    "\n",
    "    class_0 = df[df['Class'] == 0]\n",
    "    class_1 = df[df['Class'] == 1]\n",
    "    class_0_undersampled = class_0.sample(class_1.shape[0])\n",
    "\n",
    "    return pd.concat([class_0_undersampled, class_1], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df, num):\n",
    "    data = random_undersampling(df)\n",
    "    count_0, count_1 = data['Class'].value_counts()\n",
    "    return count_0 == num and count_1 == num\n",
    "\n",
    "# small data\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1],\n",
    "        [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "assert test(df1, 3)\n",
    "\n",
    "# credit card data\n",
    "assert test(credit_df, 492)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might realize the drawback of undersampling - by removing data randomly, you might have removed some valuable information from the dataset. Is there any way to do better without losing the valuable information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 1.2.2: Oversampling\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/oversampling.png\" alt=\"visualisation of oversampling\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 2: Visualisation of oversampling.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates oversampling. In oversampling, we duplicate records of the minority class such that the size of two classes balance out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2.2: Oversampling in practice\n",
    "\n",
    "Your task is to observe the data in credit\\_card.csv and apply the oversampling technique to the dataset.\n",
    "\n",
    "You might realize the bright side of oversampling - you don't lose certain valuable information. At the same time, you also realize the drawback of oversampling - it can cause overfitting and a poor generalization of the test set. Now the question is, instead of simply duplicating records, is there any other way to increase the number of records in the minority set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_oversampling(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Given credit card dataset with 0 as the majority for 'Class' column. Returns credit card data with two classes\n",
    "    having the same shape, given raw data read from csv.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The potentially imbalanced dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Oversampled dataset with equal number of fraudulent and legitimate data.\n",
    "    '''\n",
    "\n",
    "    class_0 = df[df['Class'] == 0]\n",
    "    class_1 = df[df['Class'] == 1]\n",
    "    class_1_oversampled = class_1.sample(n = class_0.shape[0], replace=True)\n",
    "\n",
    "    return pd.concat([class_0, class_1_oversampled], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(df, num):\n",
    "    data = random_oversampling(df)\n",
    "    count_0, count_1 = data['Class'].value_counts()\n",
    "    return count_0 == num and count_1 == num\n",
    "\n",
    "# small data\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1],\n",
    "        [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "assert test(df1, 6)\n",
    "\n",
    "# credit card data\n",
    "assert test(credit_df, 284315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept 1.2.3: SMOTE (for further reading only)\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/smote.png\" alt=\"visualisation of SMOTE\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 3: Visualisation of SMOTE.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The figure above illustrates Synthetic Minority Oversampling Technique (SMOTE).\n",
    "\n",
    "The SMOTE algorithm works in these four steps:\n",
    "\n",
    "1. Consider minority and majority instances in vector space.\n",
    "1. For each minority-class instance pair, interpolate their feature values.\n",
    "1. Randomly synthesize instances and label with minority class\n",
    "1. More instances added to minority class\n",
    "\n",
    "In this case, you increase the number of minority instances without simply duplicating the values. Now your newly inserted minority record is not an exact copy of an existing data point, but it is also not too different from the known observations in your minority class. Outside this problem set, when we want to do data resampling, we can use the [Python imbalanced-learn library](https://pypi.org/project/imbalanced-learn/). We will not be implementing SMOTE algorithm for this problem set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word of caution when resampling\n",
    "\n",
    "In the section above, we introduced three resampling techniques. One very important thing to note is, in practice you should first split dataset to train–test sets, then resample train and test datasets separately. This is done to avoid data leakage (snooping).\n",
    "\n",
    "Data leakage can cause you to create overly optimistic if not completely invalid predictive models. It is when information from outside the training dataset is used to train the model. The additional information might allow the model to learn something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data\n",
    "\n",
    "Before we can start training our models, we need to **randomly** partition our dataset into training data and testing data. Remember that we are trying to make a model that can predict fraudulence of data points that the model has never seen. It would be unwise to measure the accuracy of the model using the same data it trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Splitting data\n",
    "\n",
    "In this task, you need to implement `train_test_split`. This function takes `X`, `y`, and `test_size` as arguments, and output `X_train`, `X_test`, `y_train`, and `y_test` you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X: np.ndarray, y: np.ndarray, test_size: float=0.25):\n",
    "    '''\n",
    "    Randomly split the data into two sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) dataset (features).\n",
    "    y: np.ndarray\n",
    "        (m,) dataset (corresponding targets).\n",
    "    test_size: np.float64\n",
    "        fraction of the dataset to be put in the test dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of four elements (X_train, X_test, y_train, y_test):\n",
    "    X_train: np.ndarray\n",
    "        (m - k, n) training dataset (features).\n",
    "    X_test: np.ndarray\n",
    "        (k, n) testing dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m - k,) training dataset (corresponding targets).\n",
    "    y_test: np.ndarray\n",
    "        (k,) testing dataset (corresponding targets).\n",
    "    '''\n",
    "\n",
    "    m = X.shape[0]\n",
    "    k = int(m * test_size) # number of datapoints in test_set\n",
    "    y = np.reshape(y, (1, m))\n",
    "    \n",
    "    Xy = np.concatenate((X, y.T), axis = 1)\n",
    "    np.random.shuffle(Xy)\n",
    "\n",
    "    X_test = Xy[:k, :-1]\n",
    "    y_test = Xy[:k, -1]\n",
    "    X_train = Xy[k:, :-1]\n",
    "    y_train = Xy[k:, -1]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1 = [7, 2, 7, 2]\n",
    "def test_train_test_split(X, y, test_size = 0.25, expected = None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size)\n",
    "    return len(X_train) == expected[0] and len(X_test) == expected[1] and len(y_train) == expected[2] and len(y_test) == expected[3]\n",
    "assert test_train_test_split(X1, y1, expected = expected1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "Recall the gradient descent method you learned and applied in the previous problem set. Gradient descent is an iterative optimization method which finds the minimum of a differentiable function. It's often used to find the coefficients that minimize the cost function. Here is a brief recap of how that's done:\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/gradient_descent.png\" alt=\"visualisation of gradient descent\" width=\"50%\">\n",
    "</figure>\n",
    "\n",
    "[Figure 4: Visualisation of gradient descent.](https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1)\n",
    "\n",
    "We start at an initial weight vector $(w_0, w_1, ..., w_n)$. Then we take incremental steps along the steepest slope to get to the bottom where the minimum cost lies. Along the way, we keep updating the weight coefficients by computing the gradients using the training samples from the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Batch gradient descent\n",
    "\n",
    "In batch gradient descent, all the training data (entire batch) is taken into consideration in a single step. For one step of gradient descent, we calculate $- \\alpha * gradient$ based on all the training examples and then use that mean gradient to update our parameters.\n",
    "\n",
    "By continuing this for iterations until convergence, or until it's time to stop, we reach the optimal or near optimal parameters $w$ leading to minimum cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Stochastic gradient descent\n",
    "\n",
    "Think about what's a limitation of the (entire) batch gradient descent introduced earlier. What if the dataset is very large? In Stochastic Gradient Descent (SGD), we consider only the error on a single record $(x^{(*)}, y^{(*)})$ at a time. Note that SGD does not necessarily decrease the batch loss in each iteration! However, on average, the loss will still decrease. Since we work on only a single record at any time, SGD provides the advantage of a much smaller memory requirement, and faster computation time. SGD also comes with its own set of disadvantages, for example, we lose benefits of any vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: Gradient descent termination condition\n",
    "\n",
    "The next question is when to stop? The ideal case it to run until convergence, but convergence might be hard to obtain. Here are some criteria you can use:\n",
    "\n",
    "* Stop when error change is small and/or\n",
    "* Stop when error is small\n",
    "* Stop when maximum number of iterations is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to apply the batch gradient descent and stochastic gradient descent.\n",
    "Task 3.1 to 3.4 will gradually guide you to complete the logistic regression using batch gradient descent, while task 3.5 expects you to implement the stochastic one. For the task in this problem set, you can assume that the bias column has been added for all input `X`. That means given `X` as an argument, you don't need to manually add the bias column again in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: Cost function\n",
    "\n",
    "Recall that for logistic regression, we want an error function for an individual value to be:\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/piecewise_error.png\" alt=\"error function\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 5: Cost function.</figcaption>\n",
    "</figure>\n",
    "\n",
    "We can remove the condition and transform it into:\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/error.png\" alt=\"error function as a line\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 6: Cost without conditions.</figcaption>\n",
    "</figure>\n",
    "\n",
    "In this task, you need to implement `cost_function` $E$ as mentioned above. This function takes `X`, `y`, and `weight_vector` $w$ as arguments, and returns the error $E$. Note that for this task, the $E$ should account for all the trianing data.\n",
    "\n",
    "Here, we are using the log function and we need to handle the case of computing log(0). There are many ways to handle this. In this task, we will handle log(0) using machine epsilon for numpy float64 type, and use the trick log(x + EPS) which allows x to be 0. If x was any other value, log(x + EPS) would be very close to log(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X: np.ndarray, y: np.ndarray, weight_vector: np.ndarray):\n",
    "    '''\n",
    "    Cross entropy error for logistic regression\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight paramaters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Cost\n",
    "    '''\n",
    "    m, n = np.shape(X)\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    sigmoid_vectorized = np.vectorize(sigmoid)\n",
    "\n",
    "    weight_vector = np.reshape(weight_vector, (1, n))\n",
    "    y_pred = sigmoid_vectorized((weight_vector @ X.T)[0])\n",
    "    \n",
    "    EPS = np.finfo(np.float64).eps\n",
    "    cost = np.sum(-y * np.log(y_pred + EPS) - (1 - y) * np.log(1 - y_pred + EPS)) / m\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1],\n",
    "        [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([0.002, 0.1220])\n",
    "assert np.round(cost_function(X1, y1, w1), 5) == np.round(1.29333, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Weight update\n",
    "\n",
    "In this task, you need to implement `weight_update`. This function takes `X`, `y`, `alpha`, and a `weight_vector` as arguments, and output the new weight vector. Each call to the function should make one update on the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update(X: np.ndarray, y: np.ndarray, alpha: np.float64, weight_vector: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Do the weight update for one step in gradient descent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight paramaters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    New weight vector after one round of update.\n",
    "    '''\n",
    "    m, n = np.shape(X)\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    sigmoid_vectorized = np.vectorize(sigmoid)\n",
    "\n",
    "    y_pred = ([weight_vector] @ X.T)[0] # WtX\n",
    "    y_pred = sigmoid_vectorized(y_pred) # apply sigmoid function\n",
    "    weight_grad = (y_pred - y) @ X / m\n",
    "    weight_vector = weight_vector - alpha * weight_grad\n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [111.6, 10, 1],\n",
    "        [111.4, 10, 0], [111.5, 10, 1], [111.6, 10, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "w1 = np.transpose([2.2000, 12.20000])\n",
    "a1 = 1e-5\n",
    "nw1 = np.array([2.19926,12.19992])\n",
    "assert (np.round(weight_update(X1, y1, a1, w1), 5) == np.round(nw1, 5)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Logistic regression classification\n",
    "\n",
    "Remember that logistic regression is used for classification even though the function gives a probability output. In this task, you classify each element in `X`, given `weight_vector` using `prob_threshold` as the threshold, and output the classification result as an `np.ndarray`.\n",
    "\n",
    "If the probability predicted by the `weight_vector` exceeds the `prob_threshold`, we should classify it as fraud (`y = 1`). Otherwise, we should classify it as legitimate (`y = 0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classification(X: np.ndarray, weight_vector: np.ndarray, prob_threshold: np.float64=0.5):\n",
    "    '''\n",
    "    Do classification task using logistic regression.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    weight_vector: np.ndarray\n",
    "        (n,) weight paramaters.\n",
    "    prob_threshold: np.float64\n",
    "        the threshold for a prediction to be considered fraudulent.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Classification result as an (m,) np.ndarray\n",
    "    '''\n",
    "    m, n = np.shape(X)\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    sigmoid_vectorized = np.vectorize(sigmoid)\n",
    "    weight_vector = [weight_vector]\n",
    "    y_pred = sigmoid_vectorized((weight_vector @ X.T)[0])\n",
    "\n",
    "    def classify(x):\n",
    "        if x > prob_threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    classify_vectorized = np.vectorize(classify)\n",
    "    return classify_vectorized(y_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "w1 = np.transpose([-0.000002, 0.000003])\n",
    "expected1 = np.transpose([0, 0, 0, 0, 0, 0, 0, 1, 0])\n",
    "def test_classification(X, w, expected):\n",
    "    res = logistic_regression_classification(X, w)\n",
    "    return (res == expected).all() and res.shape == expected.shape\n",
    "assert test_classification(X1, w1, expected1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Logistic regression using batch gradient descent\n",
    "\n",
    "In this task, you need to implement a fixed learning rate algorithm for logistic regression. This function takes `X_train`, `y_train`, `max_num_epochs`, `threshold`, and `alpha` as arguments, and ouptut the final `weight_vector` you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_batch_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_epochs: int = 250, threshold: np.float64 = 0.05, alpha: np.float64 = 1e-5):\n",
    "    '''\n",
    "    Initialize your weight to zeros. Write your terminating condition, and run the weight update for some iterations.\n",
    "    Get the resulting weight vector. Use that to do predictions, and output the final weight vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    max_num_epochs: int\n",
    "        this should be one of the terminating condition. \n",
    "        That means if you initialize num_update_rounds to 0, \n",
    "        then you should stop updating weight when num_update_rounds >= max_num_epochs.\n",
    "    threshold: np.float64\n",
    "        terminating when error <= threshold value, or if you reach the max number of update rounds first.\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The final (n,) weight paramaters\n",
    "    '''\n",
    "    m, n = np.shape(X_train)\n",
    "\n",
    "    weight_vector = np.zeros(n)\n",
    "    error = cost_function(X_train, y_train, weight_vector)\n",
    "    num_update_rounds = 0\n",
    "\n",
    "    while num_update_rounds < max_num_epochs and error > threshold:\n",
    "        weight_vector = weight_update(X_train, y_train, alpha, weight_vector)\n",
    "        error = cost_function(X_train, y_train, weight_vector)\n",
    "        num_update_rounds += 1\n",
    "        \n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "max_num_epochs1 = 20\n",
    "expected1 = np.transpose([-0.00115, 0.00195])\n",
    "assert np.array_equal(np.round(logistic_regression_batch_gradient_descent(X1, y1, max_num_epochs1), 5), np.round(expected1, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.5: Logistic regression using stochastic gradient descent\n",
    "\n",
    "In this task, you need to implement logistic regression using stochastic gradient descent. This function takes `X_train`, `y_train`, `max_num_epochs`, `threshold`, and `alpha` as arguments, and output the final `weight_vector` you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_update_stochastic(X: np.ndarray, y: np.ndarray, alpha: np.float64, weight_vector: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Do the weight update for one step in gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (1, n) training dataset (features).\n",
    "    y: np.ndarray\n",
    "        one y in training dataset (corresponding targets).\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "    weight_vector: np.ndarray\n",
    "        (n, 1) vector of weight paramaters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    New (n,) weight paramaters after one round of update.\n",
    "    '''\n",
    "\n",
    "    m, n = np.shape(X)\n",
    "\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    sigmoid_vectorized = np.vectorize(sigmoid)\n",
    "\n",
    "    random_index = np.random.randint(0, m) # picks a random datapoint from 0 to m - 1\n",
    "    random_X = X[random_index]\n",
    "    random_y = y[random_index]\n",
    "\n",
    "    y_pred = ([weight_vector] @ random_X.T)[0] # WtX\n",
    "    y_pred = sigmoid_vectorized(y_pred) # apply sigmoid function\n",
    "    weights_grad = (y_pred - random_y) * random_X\n",
    "    weight_vector = weight_vector - alpha * weights_grad\n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_stochastic_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_epochs: int=250, threshold: np.float64=0.05, alpha: np.float64=1e-5) -> np.ndarray:\n",
    "    '''\n",
    "    Initialize your weight to zeros. Write a terminating condition, and run the weight update for some iterations.\n",
    "    Get the resulting weight vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    max_num_epochs: int\n",
    "        this should be one of the terminating conditions. \n",
    "        The gradient descent step should happen at most max_num_epochs times.\n",
    "    threshold: np.float64\n",
    "        terminating when error <= threshold value, or if you reach the max number of update rounds first.\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The final (n,) weight paramaters\n",
    "    '''\n",
    "\n",
    "    m, n = np.shape(X_train)\n",
    "\n",
    "    weight_vector = np.zeros(n)\n",
    "    error = cost_function(X_train, y_train, weight_vector)\n",
    "    num_update_rounds = 0\n",
    "\n",
    "    while num_update_rounds < max_num_epochs and error > threshold:\n",
    "        weight_vector = weight_update_stochastic(X_train, y_train, alpha, weight_vector)\n",
    "        error = cost_function(X_train, y_train, weight_vector)\n",
    "        num_update_rounds += 1\n",
    "        \n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1 = cost_function(X1, y1, np.transpose(np.zeros(X1.shape[1])))\n",
    "def test_stochastic(X, y, expected):\n",
    "    return cost_function(X, y, logistic_regression_stochastic_gradient_descent(X, y)) < expected\n",
    "assert test_stochastic(X1, y1, expected1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.6: Stochastic gradient descent vs batch gradient descent\n",
    "\n",
    "Think about the number of iterations and/or the runtime it takes to run the update in task 3.4 and task 3.5. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.7: Plotting\n",
    "\n",
    "Make a plot you think will be relevant in supporting your observation in 3.6. You can create helper functions if necessary. For your plot, please include the x and y labels. Put both methods (batch gradient descent and stochastic gradient descent) in one diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbb5b9b4df0>"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def stochastic_loss_per_iteration(X_train: np.ndarray, y_train: np.ndarray, max_num_epochs: int=250, threshold: np.float64=0.05, alpha: np.float64=1e-5) -> np.ndarray:\n",
    "\n",
    "    # this function performs 250 iterations and records the loss at every iteration\n",
    "\n",
    "    m, n = np.shape(X_train)\n",
    "\n",
    "    weight_vector = np.zeros(n)\n",
    "    error = cost_function(X_train, y_train, weight_vector)\n",
    "    num_update_rounds = 0\n",
    "\n",
    "    loss_array = []\n",
    "\n",
    "    while num_update_rounds < max_num_epochs:\n",
    "        weight_vector = weight_update_stochastic(X_train, y_train, alpha, weight_vector)\n",
    "        error = cost_function(X_train, y_train, weight_vector)\n",
    "        loss_array.append(error)\n",
    "        num_update_rounds += 1\n",
    "        \n",
    "    return loss_array\n",
    "\n",
    "def batch_loss_per_iteration(X_train: np.ndarray, y_train: np.ndarray, max_num_epochs: int = 250, threshold: np.float64 = 0.05, alpha: np.float64 = 1e-5):\n",
    "\n",
    "    # this function performs 250 iterations and records the loss at every iteration\n",
    "\n",
    "    m, n = np.shape(X_train)\n",
    "\n",
    "    weight_vector = np.zeros(n)\n",
    "    error = cost_function(X_train, y_train, weight_vector)\n",
    "    num_update_rounds = 0\n",
    "    \n",
    "    loss_array = []\n",
    "\n",
    "    while num_update_rounds < max_num_epochs:\n",
    "        weight_vector = weight_update(X_train, y_train, alpha, weight_vector)\n",
    "        error = cost_function(X_train, y_train, weight_vector)\n",
    "        loss_array.append(error)\n",
    "        num_update_rounds += 1\n",
    "        \n",
    "    return loss_array\n",
    "\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "\n",
    "stochastic_loss = stochastic_loss_per_iteration(X1, y1, 250)\n",
    "batch_loss = batch_loss_per_iteration(X1, y1, 250)\n",
    "num_iterations = [i for i in range(1, 251)]\n",
    "\n",
    "plt.title('Plot of loss over time')\n",
    "plt.xlabel('number of iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(num_iterations, stochastic_loss, label = 'Stochastic Gradient Descent')\n",
    "plt.plot(num_iterations, batch_loss, label = 'Batch Gradient Descent')\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class logistic regression\n",
    "\n",
    "Now that you have helped the credit card companies recognize fraudulent transactions, customers are less likely to be charged for items that they did not purchase. They are very satisfied with how well your model performed. They want to offer you a meal, at a restaurant of your choice. You decide to make your choice based on the occupancy of the restaurant. Luckily for you, you have some starting data `restaurant_data.csv` to work with. The dataset provides three input features \"max_capacity\", \"feedback_score\", and \"average_expense\", and an ouptut feature \"occupancy\" that takes the value of \"none\", \"some\", or \"full\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.1: Multi-class logistic regression using batch gradient descent\n",
    "\n",
    "In this task, you need to implement a fixed learning rate algorithm for multi-class logistic regression. This function takes `X_train`, `y_train`, `max_num_epochs`, `threshold`, `alpha`, and `class_i` as arguments, and output the final `weight_vector` you obtained. Here, argument `class_i` is the class that you want to train in this round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_logistic_regression_batch_gradient_descent(X_train: np.ndarray, y_train: np.ndarray, max_num_epochs: int, threshold: np.float64, alpha: np.float64, class_i: str) -> np.ndarray:\n",
    "    '''\n",
    "    Initialize your weight to zeros. Write your terminating condition, and run the weight update for some iterations.\n",
    "    Get the resulting weight vector. Output the resulting weight vector.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: np.ndarray\n",
    "        (m, n) training dataset (features).\n",
    "    y_train: np.ndarray\n",
    "        (m,) training dataset (corresponding targets).\n",
    "    alpha: np.float64\n",
    "        logistic regression learning rate.\n",
    "    max_num_epochs: int\n",
    "        this should be one of the terminating condition. \n",
    "        That means if you initialize num_update_rounds to 0, \n",
    "        then you should stop updating weight when num_update_rounds >= max_num_epochs.\n",
    "    threshold: float\n",
    "        terminating when error <= threshold value, or if you reach the max number of update rounds first.\n",
    "    class_i: string\n",
    "        one of 'none', 'full', 'some'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The final (n,) weight paramaters\n",
    "    '''\n",
    "\n",
    "    def none_vs_all(x):\n",
    "        if x == 'none':\n",
    "            return 1\n",
    "        return 0\n",
    "    none_vectorized = np.vectorize(none_vs_all)\n",
    "    \n",
    "    def full_vs_all(x):\n",
    "        if x == 'full':\n",
    "            return 1\n",
    "        return 0\n",
    "    full_vectorized = np.vectorize(full_vs_all)\n",
    "    \n",
    "    def some_vs_all(x):\n",
    "        if x == 'some':\n",
    "            return 1\n",
    "        return 0\n",
    "    some_vectorized = np.vectorize(some_vs_all)\n",
    "\n",
    "    if class_i == 'none':\n",
    "        y_train = none_vectorized(y_train)\n",
    "    elif class_i == 'full':\n",
    "        y_train = full_vectorized(y_train)\n",
    "    elif class_i == 'some':\n",
    "        y_train = some_vectorized(y_train)\n",
    "        \n",
    "    return logistic_regression_batch_gradient_descent(X_train, y_train, max_num_epochs, threshold, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[26, 9, 69, 'full'],\n",
    "        [54, 3, 16, 'some'],\n",
    "        [59, 7, 50, 'some' ],\n",
    "        [33, 0, 45, 'full']]\n",
    "df1 = pd.DataFrame(data1, columns = ['max_capcity', 'feedback_score', 'average_expense', 'occupancy'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "max_num_epochs1 = 20\n",
    "expected1 = np.transpose([6.75, 0.125, -6.0])\n",
    "assert np.array_equal(multi_class_logistic_regression_batch_gradient_descent(X1, y1, max_num_epochs1, 0.05, 1, 'some'), expected1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2: Do classification on multi-class problem\n",
    "\n",
    "Finally, you classify each element in `X`, given three weight vectors `weight_vector_none`, `weight_vector_some`, and `weight_vector_full`, and output a list of classification results in the form of an array (`['some', 'none', 'full', 'none', ...]`). Note that in the case of a tie, you can choose the class in the order of priority: `'none'`, `'some'`, `'full'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_logistic_regression_classification(X: np.ndarray, weight_vector_none: np.ndarray, weight_vector_some: np.ndarray, weight_vector_full: np.ndarray):\n",
    "    '''\n",
    "    Do classification task using logistic regression.\n",
    "    In the case of a tie, break the tie in the priority 'none' > 'some' > 'full'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) training dataset (features)\n",
    "    weight_vector_none: np.ndarray\n",
    "        (n,) weight paramaters for the 'none' class.\n",
    "    weight_vector_some: np.ndarray\n",
    "        (n,) weight paramaters for the 'some' class.\n",
    "    weight_vector_full: np.ndarray\n",
    "        (n,) weight paramaters for the 'full' class.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Classification result as an (m,) np.ndarray. Eg ['some', 'none', 'full', ... ,'none'].\n",
    "    '''\n",
    "\n",
    "    def get_probabilities(X, weight_vector):\n",
    "        \n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        sigmoid_vectorized = np.vectorize(sigmoid)\n",
    "\n",
    "        weight_vector = [weight_vector] # 1 x n\n",
    "        y_pred = weight_vector @ X.T\n",
    "        y_pred = sigmoid_vectorized(y_pred)\n",
    "        return y_pred\n",
    "\n",
    "    none_probabilities = get_probabilities(X, weight_vector_none)\n",
    "    some_probabilities = get_probabilities(X, weight_vector_some)\n",
    "    full_probabilities = get_probabilities(X, weight_vector_full)\n",
    "\n",
    "    concat_probabilities = np.concatenate((none_probabilities, some_probabilities, full_probabilities), axis = 0)\n",
    "    final_labels = np.argmax(concat_probabilities, axis = 0) # final labels in the form [1, 0, 1, 2] where 0 is none, 1 is some, 2 is full\n",
    "    \n",
    "    def labels_to_string(x):\n",
    "        if x == 0:\n",
    "            return 'none'\n",
    "        elif x == 1:\n",
    "            return 'some'\n",
    "        elif x == 2:\n",
    "            return 'full'\n",
    "    labels_to_string_vectorized = np.vectorize(labels_to_string)\n",
    "\n",
    "    return labels_to_string_vectorized(final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = [[26, 9, 69, 'full'],\n",
    "        [54, 3, 16, 'some'],\n",
    "        [59, 7, 50, 'some' ],\n",
    "        [33, 0, 45, 'full']]\n",
    "df1 = pd.DataFrame(data1, columns = ['max_capcity', 'feedback_score', 'average_expense', 'occupancy'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "\n",
    "w11 = np.transpose([0.0013351567670329624, 2.5757816929896605e-05, -0.001189020140476165])\n",
    "w12 = np.transpose([2.5757816929896605e-05, -0.001189020140476165, 0.0013351567670329624])\n",
    "w13 = np.transpose([2.5757816929896605e-05, 0.0013351567670329624, -0.001189020140476165])\n",
    "expected1 = np.transpose(['some', 'none', 'some', 'some'])\n",
    "def test_multi_class_classification(X, w1, w2, w3, expected):\n",
    "    res = multi_class_logistic_regression_classification(X, w1, w2, w3)\n",
    "    return res.shape == expected.shape and (res == expected).all()\n",
    "assert test_multi_class_classification(X1, w11, w12, w13, expected1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine\n",
    "\n",
    "Now let's apply the support vector machine we learnt in lecture to the credit card dataset. Here is a quick recap of what SVM is about: We are given a training dataset of $n$ points of the form $(x^{(i)}, y^{(i)})$, where the $y^{(i)}$ are either 1 or 0, each indicating the class to which the point $x^{(i)}$ belongs.\n",
    "\n",
    "Each $x^{(i)}$ n-dimensional real vector. We want to find the \"maximum-margin hyperplane\" that divides the group of points $x^{(i)}$ for which $y_{i}=1$ from the group of points for which $y_{i}=0$, which is defined so that the distance between the hyperplane and the nearest point $x^{(i)}$ from either group is maximized.\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/svm.png\" alt=\"visualisation of support vector machine\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 7: Visualisation of support vector machine.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Then how should we maximize the distance between the data points and hyperplane? To do that, we use hinge loss. We also add a regularization parameter the cost function to balance the margin maximization and loss. Now let's start on implementing the SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.1: Linear SVM\n",
    "\n",
    "Take a look at the Python library [scikit-learn](https://scikit-learn.org/stable/modules/svm.html) and import it for your use. In this task, the function takes `X`, `y`. Note that here `X` and `y` are not training set but the entire dataset, so you are asked to do a train test data split with `test_size` of 0.3, and `random_state` of 42, **using what's provided in the library** (not what you implemented in task 2.1). Then, create an instance of a Linear SVM classifier using the default parameters, train the Linear SVM classifier, and output the predictions as well as the accuracy score. For this task, you should make full use of the library functions, instead of doing computations on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "def linear_svm(X: np.ndarray, y: np.ndarray):\n",
    "    '''\n",
    "    Do classification using linear svm. Given X and y, note that here X and y are not training sets, but rather the\n",
    "    entire dataset. Do a train test data split with test_size=0.3, and random_state=42.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) whole dataset (features)\n",
    "    y: np.ndarray\n",
    "        (m,) whole dataset (corresponding targets)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred: np.ndarray\n",
    "        The predictions.\n",
    "    acc: np.float64\n",
    "        The accuracy on a scale up to 100.\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "    return y_pred, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(X, y, expected, expected_accuracy):\n",
    "    result = linear_svm(X, y)\n",
    "    return (result[0] == expected).all() and (result[0]).shape == expected.shape and round(result[1], 5) == round(expected_accuracy, 5)\n",
    "\n",
    "# small data\n",
    "data1 = [[111.1, 10, 0], [111.2, 20, 0], [111.3, 10, 0], [111.4, 10, 0], [111.5, 10, 0], [211.6, 80, 1],\n",
    "        [111.4, 10, 0], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1_y = np.transpose([0, 0, 1])\n",
    "expected1_accuracy = 66.66666666666666\n",
    "assert test_classification(X1, y1, expected1_y, expected1_accuracy)\n",
    "\n",
    "# subset of credit card data\n",
    "class_0 = credit_df[credit_df['Class'] == 0]\n",
    "class_1 = credit_df[credit_df['Class'] == 1]\n",
    "\n",
    "data_0 = class_0.sample(n=15, random_state=42)\n",
    "data_1 = class_1.sample(n=50, random_state=42)\n",
    "data_100 = pd.concat([data_1, data_0], axis=0)\n",
    "X = data_100.iloc[:, :-1].to_numpy()\n",
    "y = data_100.iloc[:, -1].to_numpy()\n",
    "\n",
    "expected_pred = np.transpose([1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "expected_accuracy = 80.0\n",
    "assert test_classification(X, y.ravel(), expected_pred, expected_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.2: Gaussian Kernel SVM\n",
    "\n",
    "Take a look at the Python library [scikit-learn](https://scikit-learn.org/stable/modules/svm.html) and import it for your use. In this task, the function takes `X`, `y`. Note that here `X` and `y` are not training set but the entire dataset, so you are asked to do a train test data split with `test_size` of 0.3, and `random_state` of 42, using what's provided in the library. Then, create an instance of a Gaussian Kernel SVM classifier using the default parameters, train the Gaussian Kernel SVM classifier, and output the predictions as well as the accuracy score. For this task, you should make full use of the library functions, instead of doing computations on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_svm(X: np.ndarray, y: np.ndarray):\n",
    "    '''\n",
    "    Do classification using Gaussian Kernel svm. Given X and y, note that here X and y are not training sets, but\n",
    "    rather the entire dataset. Do a train test data split with test_size=0.3, and random_state=42.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray\n",
    "        (m, n) whole dataset (features)\n",
    "    y: np.ndarray\n",
    "        (m,) whole dataset (corresponding targets)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred: np.ndarray\n",
    "        The predictions.\n",
    "    acc: np.float64\n",
    "        The accuracy on a scale up to 100.\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred) * 100\n",
    "\n",
    "    return y_pred, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classification(X, y, expected, expected_accuracy):\n",
    "    result = gaussian_kernel_svm(X, y)\n",
    "    return (result[0] == expected).all() and (result[0]).shape == expected.shape and round(result[1], 5) == round(expected_accuracy, 5)\n",
    "\n",
    "# small data\n",
    "data1 = [[111.1, 10, -1], [111.2, 20, -1], [111.3, 10, -1], [111.4, 10, -1], [111.5, 10, -1], [211.6, 80, 1],\n",
    "        [111.4, 10, -1], [111.5, 80, 1], [211.6, 80, 1]]\n",
    "df1 = pd.DataFrame(data1, columns = ['V1', 'V2', 'Class'])\n",
    "X1 = df1.iloc[:, :-1].to_numpy()\n",
    "y1 = df1.iloc[:, -1].to_numpy()\n",
    "expected1_y = np.transpose([-1, -1, 1])\n",
    "expected1_accuracy = 66.66666666666666\n",
    "assert test_classification(X1, y1, expected1_y, expected1_accuracy)\n",
    "\n",
    "# subset of credit card data\n",
    "class_0 = credit_df[credit_df['Class'] == 0]\n",
    "class_1 = credit_df[credit_df['Class'] == 1]\n",
    "\n",
    "data_0 = class_0.sample(n=15, random_state=42)\n",
    "data_1 = class_1.sample(n=50, random_state=42)\n",
    "data_100 = pd.concat([data_1, data_0], axis=0)\n",
    "X = data_100.iloc[:, :-1].to_numpy()\n",
    "y = data_100.iloc[:, -1].to_numpy()\n",
    "\n",
    "expected_pred = np.transpose([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "expected_accuracy = 80.0\n",
    "assert test_classification(X, y.ravel(), expected_pred, expected_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5.3: Linear SVM vs Gaussian Kernel SVM\n",
    "\n",
    "Based on your observation, when using support vector machine, how do you think we should choose linear kernel vs Gaussian Kernel? In other words, which kernel is better in what cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you are done, please submit your work to Coursemology, by copying the right snippets of code into the corresponding box that says \"Your answer,\"and click \"Save.\" After you save, you can still make changes to your submission.\n",
    "\n",
    "Once you are satisfied with what you have uploaded, click \"Finalize submission.\" Note that once your submission is finalized, it is considered to be submitted for grading and cannot be changed. If you need to undo this action, you will have to email your assigned tutor for help. Please do not finalize your submission until you are sure that you want to submit your solutions for grading.\n",
    "\n",
    "*Have fun and enjoy coding.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
