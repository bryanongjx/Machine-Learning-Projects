{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Predicting House Prices in Singapore\n",
    "\n",
    "**Release Date:** 9 September 2022\n",
    "\n",
    "**Due Date:** 23:59, 24 September 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We have learned how to solve a regression problem using linear regression in class.\n",
    "In this problem set, we will apply our knowledge to solve a real-world problem. More\n",
    "specifically, we will develop linear regression and polynomial regression models to predict\n",
    "house prices in Singapore.\n",
    "\n",
    "**Required Files**:\n",
    "* ps3.ipynb\n",
    "* housing_data.csv\n",
    "\n",
    "**Honour Code**: Note that plagiarism will not be condoned! You may discuss with your classmates and check the internet for references, but you MUST NOT submit code/report that is copied directly from other sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orientation to files\n",
    "\n",
    "**ps3.ipynb**:\n",
    "The template for all your tasks is provided in this file. Some test cases have\n",
    "been provided for you to check the output of your algorithm against the expected result. The tests are **not** comprehensive, and you are\n",
    "encouraged to write your own tests to check for correctness.\n",
    "\n",
    "**housing.csv**:\n",
    "There are 90 housing data points. Each data point consists of 3 features:\n",
    "* **floor_area_sqm** - size of the house in square meters\n",
    "* **bedrooms** - number of bedrooms\n",
    "* **schools** - number of primary schools within 1km radius\n",
    "\n",
    "Our target value is the **asking_price**, which is the price of the house unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT\n",
    "\n",
    "Similar to PS0, your implementation in the following tasks **should not\n",
    "involve any iteration, including `map` and `filter`, or recursion**. Instead, please work\n",
    "with the operations available in NumPy<sup>&#x2020;</sup>. Solutions that violate this will be penalised.\n",
    "\n",
    "There is however, an exception for **Tasks 2.4, 2.5 and 3.4**. In the pseudo-code for the\n",
    "algorithm required, there is an explicit while loop. Hence, **only for these tasks**, you\n",
    "may use a **single for/while loop** to iterate for the number of iterations required.\n",
    "\n",
    "<sup>&#x2020;</sup> You are allowed to use any mathematical functions, but this **does not mean that you are allowed to\n",
    "use *any* NumPy function** (there are NumPy functions that aren’t mathematical functions). For example,\n",
    "`np.vectorize` is not allowed since it is iterative. If you are in doubt about which functions are allowed, please\n",
    "ask in the forum (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inital imports and setup\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "###################\n",
    "# Helper function #\n",
    "###################\n",
    "def load_data(filepath):\n",
    "    '''\n",
    "    Load in the given csv filepath as a numpy array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath (string) : path to csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        X, y (np.ndarray, np.ndarray) : (m, num_features), (m,) numpy matrices\n",
    "    '''\n",
    "    *X, y = np.genfromtxt(\n",
    "        filepath,\n",
    "        delimiter=',',\n",
    "        skip_header=True,\n",
    "        unpack=True,\n",
    "    ) # default dtype: float\n",
    "    X = np.array(X, dtype=float).T # cast features to int type\n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "data_filepath = 'housing_data.csv'\n",
    "X, y = load_data(data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Defining cost functions\n",
    "\n",
    "We need to define cost functions before creating a linear regression model to calculate\n",
    "the error between our prediction and the target value. We will define two cost functions:\n",
    "Mean Squared Error (MSE) and Mean Absolute Error (MAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Mean Squared Error (MSE)\n",
    "\n",
    "Write the function `mean_squared_error(y_target, y_pred)` that returns a number representing the mean squared error of the predictions.\n",
    "\n",
    "The formula of Mean Squared Error is as follows:\n",
    "$$ MSE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y}_i - y_i)^2 $$\n",
    "\n",
    "where $\\boldsymbol{y}$ is the target vector, $\\boldsymbol{\\hat{y}}$ is the prediction vector, and $m$ is number of samples in the\n",
    "training data.\n",
    "\n",
    "**Remark**: The formula here follows the lecture slides for consistency. In definitions and implementations elsewhere, the denominator is usually just $m$ instead of $2m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_target, y_pred):\n",
    "    '''\n",
    "    Calculate mean squared error between y_pred and y_target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_target (np.ndarray) : (m, 1) numpy matrix consists of target values\n",
    "    y_pred (np.ndarray)   : (m, 1) numpy matrix consists of predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The mean squared error value.\n",
    "    '''\n",
    "    m, n = y_target.shape\n",
    "    result = np.sum(np.square(y_pred - y_target))\n",
    "    return result / (2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target, y_pred = np.array([[3], [5]]), np.array([[12], [15]])\n",
    "\n",
    "assert mean_squared_error(y_target, y_pred) == 45.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Mean Absolute Error (MAE)\n",
    "\n",
    "Write the function `mean_absolute_error(y_target, y_pred)` that returns a number representing the mean absolute error of the predictions.\n",
    "\n",
    "The formula of Mean Absolute Error is as follows:\n",
    "$$ MSE(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}) = \\frac{1}{m} \\sum_{i=1}^{m}|\\hat{y}_i - y_i| $$\n",
    "\n",
    "where $\\boldsymbol{y}$ is the target vector, $\\boldsymbol{\\hat{y}}$ is the prediction vector, and $m$ is number of samples in the\n",
    "training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_target, y_pred):\n",
    "    '''\n",
    "    Calculate mean absolute error between y_pred and y_target.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_target (np.ndarray) : (m, 1) numpy matrix consists of target values\n",
    "    y_pred (np.ndarray)   : (m, 1) numpy matrix consists of predictions\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        The mean absolute error value.\n",
    "    '''\n",
    "  \n",
    "    m, n = np.shape(y_target)\n",
    "\n",
    "    result = np.sum(np.abs(y_pred - y_target))\n",
    "    return result / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target, y_pred = np.array([[3], [5]]), np.array([[12], [15]])\n",
    "\n",
    "assert mean_absolute_error(y_target, y_pred) == 9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression\n",
    "\n",
    "Now we’re ready to create our own linear regression model. We will try to approximate a linear function, which can be written as follows:\n",
    "\n",
    "$$ y = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n $$\n",
    "\n",
    "where $y$ is the target value, $x_1, x_2, \\dots, x_n$ are feature values, and $w_0, w_1, \\dots, w_n$ are parameters. $w_0$ is meant to represent the bias term, while $w_1, \\dots, w_n$ are the feature weights.\n",
    "\n",
    "**Bias term**\n",
    "\n",
    "The bias term ($w_0$) is useful in capturing an inherent offset of the target values from the origin, i.e. they have some non-zero \"default\" or \"starting\" value. The bias term accounts for this default value in our model. Without a bias term (or bias = 0), our regression lines will pass through the origin, which might not be appropriate for the data in question.\n",
    "\n",
    "Consider the scatter plot below. The blue line is the best fitting line without a bias term, while the red line includes a non-zero bias. Since the blue line starts at the origin, it is unable to capture the offset of the points. In contrast, the red line  starts higher (at around 5), and hence is better able to approximate the data.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"imgs/bias_scatter.png\" alt=\"bias vs no bias\" width=\"40%\">\n",
    "<figcaption style=\"text-align:center\">Figure 0: Example of models with bias vs without bias.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Usually, we have to explicitly add a bias term into our data when building our models. In the following tasks, you'll explore how to do so, and how this choice can affect the accuracy of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Adding a bias column\n",
    "\n",
    "In the lecture, we learned that adding a bias column allows our linear model to be more\n",
    "flexible. Write the function `add_bias_column(X)` that takes a NumPy matrix `X` and returns\n",
    "a new matrix with an additional column. The additional column should have all its\n",
    "elements set to be 1 and is located at the first column of the matrix.\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/add_bias.jpeg\" alt=\"adding bias\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 1: Example of a matrix before and after adding a bias column.</figcaption>\n",
    "</figure>\n",
    "\n",
    "**Note**: Your function should work for all kinds of matrix shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias_column(X):\n",
    "    '''\n",
    "    Create a bias column and combine it with X.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : (m, n) numpy matrix representing a feature matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        new_X (np.ndarray):\n",
    "            A (m, n + 1) numpy matrix with the first column consisting of all 1s\n",
    "    '''\n",
    "  \n",
    "    m, n = np.shape(X)\n",
    "\n",
    "    bias = np.ones((m, 1))\n",
    "    res = np.concatenate((bias, X), axis = 1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_bias = np.array([[1, 2], [3, 4]])\n",
    "expected = np.array([[1, 1, 2], [1, 3, 4]])\n",
    "\n",
    "assert np.array_equal(add_bias_column(without_bias), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Get best fitting bias and weights\n",
    "\n",
    "Write the function `get_bias_and_weight(X, y, include_bias)` that returns $w_0$ (bias) and\n",
    "$w_1, w_2, \\dots, w_n$ (weights) that will lead to best fitting line.\n",
    "\n",
    "The `include_bias` argument is used to specify if the model includes a bias term, i.e. has a non-zero bias term. Hence, the function should return $w_0 = 0$ if it is set to be `false`. The function should return $w_1, \\dots, w_n$ as a NumPy matrix with shape $(n, 1)$, where $n$ is the number of features (excluding the bias column).\n",
    "\n",
    "We can use the normal equation to get $w_0, w_1, \\dots, w_n$. The normal equation is as\n",
    "follows:\n",
    "\n",
    "$$ \\begin{pmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_n \\end{pmatrix} = (X^TX)^{-1}X^T \\boldsymbol{y} $$\n",
    "\n",
    "where $X$ is the (augmented for bias) feature matrix and $\\boldsymbol{y}$ is the vector of target values.\n",
    "\n",
    "**Note**: You can use `add_bias_column` function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias_and_weight(X, y, include_bias = True):\n",
    "    '''\n",
    "    Calculate bias and weights that give the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            If include_bias = True, return the bias constant. Else,\n",
    "            return 0\n",
    "        weights (np.ndarray):\n",
    "            A (n, 1) numpy matrix representing the weight constant(s).\n",
    "    '''\n",
    "  \n",
    "    # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "    if include_bias:\n",
    "        X = add_bias_column(X)\n",
    "\n",
    "    weight_matrix = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    print(weight_matrix)\n",
    "    if include_bias:\n",
    "        return weight_matrix[0][0], weight_matrix[1:]\n",
    "    else:\n",
    "        return 0, weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.]\n",
      " [1.]]\n",
      "[[3.]\n",
      " [1.]]\n",
      "[[0.48571429]\n",
      " [1.2       ]]\n"
     ]
    }
   ],
   "source": [
    "public_X1, public_y1 = np.array([[1], [2], [3]]), np.array([[4], [5], [6]])\n",
    "public_X2, public_y2 = np.array([[1, 3], [2, 3], [3, 4]]), np.arange(4, 7).reshape((-1, 1))\n",
    "\n",
    "test_1 = (round(get_bias_and_weight(public_X1, public_y1)[0], 5) == 3)\n",
    "test_2 = (np.round(get_bias_and_weight(public_X1, public_y1)[1], 1) == np.array([[1.0]]))\n",
    "test_3 = np.array_equal(np.round(get_bias_and_weight(public_X2, public_y2, False)[1], 2), np.round(np.array([[0.49], [1.20]]), 2))\n",
    "\n",
    "assert test_1 and test_2 and test_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Get the prediction line\n",
    "\n",
    "Write the function `get_prediction_linear_regression(X, y, include_bias)` that returns `y_pred`,\n",
    "a vector of predicted values for the training data.\n",
    "\n",
    "**Note**: You can use `get_bias_and_weight` function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_linear_regression(X, y, include_bias = True):\n",
    "    '''\n",
    "    Calculate the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        y_pred (np.ndarray):\n",
    "            A (m, 1) numpy matrix representing prediction values.\n",
    "    '''\n",
    "  \n",
    "    bias, weights = get_bias_and_weight(X, y, include_bias)\n",
    "\n",
    "    y_pred = X @ weights + bias\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [3.90798505e-14]]\n"
     ]
    }
   ],
   "source": [
    "test_X, test_y = np.array([[1, 3], [2, 3], [3, 4]]), np.arange(4, 7).reshape((-1, 1))\n",
    "\n",
    "assert round(mean_squared_error(test_y, get_prediction_linear_regression(test_X, test_y)), 5) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, create a linear regression model with **floor_area_sqm** as the only\n",
    "feature and **asking_price** as the target value. Plot your prediction line using the code\n",
    "snippet below. It should look similar to Figure 2.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"imgs/linear_reg.png\" alt=\"regression plot\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 2: Example of linear regression using <b>floor_area_sqm</b> as feature.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42834.57789779]\n",
      " [ 5500.84869261]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "area = X[:, 0].reshape((-1, 1))\n",
    "predicted = get_prediction_linear_regression(area, y)\n",
    "plt.scatter(area, y)\n",
    "plt.plot(area, predicted, color = 'r')\n",
    "plt.xlabel(\"Size in square meter\")\n",
    "plt.ylabel(\"Price in SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We will now learn to use gradient descent to approximate $\\boldsymbol{w} = w_0, w_1, \\dots, w_n$.\n",
    "\n",
    "*Gradient descent*<sup>&#x2020;</sup> is an algorithm that minimizes the cost function by trying to iteratively\n",
    "find the best parameters. In linear regression, we will try to minimize the Mean Squared\n",
    "Error. The outline of the algorithm is as follows:\n",
    "    \n",
    "* Start with some $\\boldsymbol{w} = (w_0, \\dots, w_n)$\n",
    "* Keep changing $w_0,\\dots, w_n$ to minimize $J(\\boldsymbol{w})$, where $J$ is our cost function\n",
    "\n",
    "In this problem set, we will initially set $w_0, w_1, \\dots, w_n$ to all be 0s. Then, we will set a\n",
    "learning rate $\\alpha$ that will affect the rate of change of $w_0, \\dots, w_n$. Lastly, we will set\n",
    "$N$ to specify the number of iterations of gradient descent we want to run.\n",
    "\n",
    "The pseudo-code of Gradient Descent for linear regression is defined in Algorithm 1.\n",
    "\n",
    "**Note**: In the following gradient descent related tasks, calculate the value of loss function *after* updating the bias and weights.\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/mse_plot.png\" alt=\"gradient descent\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 3: Gradient descent tries to find the parameters that leads to lowest MSE.</figcaption>\n",
    "</figure>\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"imgs/grad_desc_algorithm.png\" alt=\"gradient descent\" width=\"100%\">\n",
    "</figure>\n",
    "\n",
    "<sup>&#x2020;</sup> Gradient Descent algorithm is not limited to the linear regression model, but is a general optimisation technique, and is also used in many other machine learning models such as Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Gradient Descent on a single feature\n",
    "\n",
    "Write the function `gradient_descent_one_variable(x, y, lr, number_of_iterations)` that\n",
    "returns:\n",
    "\n",
    "* $w_0$ - a number representing the bias constant\n",
    "* $w_1$ - a number representing the weight constant\n",
    "* $loss$ - a list that contains the MSE scores calculated during the gradient descent process.\n",
    "\n",
    "The default value is $10^{-5}$ for `lr` and $250$ for `number_of_iterations`.\n",
    "\n",
    "**Note**: You can use `mean_squared_error` function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_one_variable(x, y, lr = 1e-5, number_of_iterations = 250):\n",
    "    '''\n",
    "    Approximate bias and weight that give the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x (np.ndarray) : (m, 1) numpy matrix representing a feature column\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    lr (float) : Learning rate\n",
    "    number_of_iterations (int) : Number of gradient descent iterations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            The bias constant\n",
    "        weight (float):\n",
    "            The weight constant\n",
    "        loss (list):\n",
    "            A list where the i-th element denotes the MSE score at i-th iteration.\n",
    "    '''\n",
    "    # Do not change\n",
    "    bias = 0\n",
    "    weight = 0\n",
    "    loss = []\n",
    "\n",
    "    m, n = np.shape(x)\n",
    "\n",
    "    for _ in range(number_of_iterations):\n",
    "        # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "        y_pred = x * weight + bias\n",
    "        l = mean_squared_error(y, y_pred)\n",
    "        loss.append(l)\n",
    "\n",
    "        w0_grad = np.sum((y_pred - y)) / m\n",
    "        w1_grad = np.sum((y_pred - y) * x) / m\n",
    "\n",
    "        temp0 = bias - lr * w0_grad\n",
    "        temp1 = weight - lr * w1_grad\n",
    "        bias = temp0\n",
    "        weight = temp1\n",
    "    \n",
    "    return bias, weight, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = X[:, 0].reshape((-1, 1))\n",
    "\n",
    "loss_initial = gradient_descent_one_variable(area, y, lr = 1e-5, number_of_iterations = 250)[2][0]\n",
    "loss_final = gradient_descent_one_variable(area, y, lr = 1e-5, number_of_iterations = 250)[2][-1]\n",
    "\n",
    "assert loss_initial > loss_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, plot the `loss` against `number_of_iterations` using the code snippet\n",
    "below. It should be similar to Figure 4.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"imgs/loss.png\" alt=\"gradient descent\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 4: MSE values decrease as the iteration number increases.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "area = X[:, 0].reshape((-1, 1))\n",
    "b, w, loss = gradient_descent_one_variable(area, y, 1e-5, 250)\n",
    "plt.plot([i for i in range(len(loss))], loss)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Gradient Descent on multiple features\n",
    "\n",
    "Now, extend the code in Task 2.4 so it is able to find $w_0, w_1, \\dots, w_n$ for multiple features. Write the function `gradient_descent_multi_variable(X, y, lr, number_of_iterations)` that returns:\n",
    "\n",
    "* $w_0$ - a number representing the bias constant\n",
    "* $w_1, w_2, \\dots, w_n$ - $(n,1)$ NumPy matrix, where each element denotes the weight constant of a certain feature\n",
    "* $loss$ - a list that contains the MSE scores calculated during the gradient descent process.\n",
    "\n",
    "**Note**: You can use `mean_squared_error` function for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_multi_variable(X, y, lr = 1e-5, number_of_iterations = 250):\n",
    "    '''\n",
    "    Approximate bias and weight that gave the best fitting line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    lr (float) : Learning rate\n",
    "    number_of_iterations (int) : Number of gradient descent iterations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            The bias constant\n",
    "        weights (np.ndarray):\n",
    "            A (n, 1) numpy matrix that specifies the weight constants.\n",
    "        loss (list):\n",
    "            A list where the i-th element denotes the MSE score at i-th iteration.\n",
    "    '''\n",
    "    # Do not change\n",
    "    bias = 0\n",
    "    weights = np.full((X.shape[1], 1), 0).astype(float)\n",
    "    loss = []\n",
    "    m, n = X.shape\n",
    "\n",
    "    for _ in range(number_of_iterations):\n",
    "        # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "        y_pred = X @ weights + bias\n",
    "        l= mean_squared_error(y, y_pred)\n",
    "        loss.append(l)\n",
    "\n",
    "        grad_bias = np.sum(y_pred - y) / m\n",
    "        grad_weights = np.sum(((y_pred - y) * X), axis = 0) / m\n",
    "\n",
    "        temp_bias = bias - lr * grad_bias\n",
    "        temp_weights = weights - lr * grad_weights\n",
    "        \n",
    "        bias = temp_bias\n",
    "        weights = temp_weights\n",
    "\n",
    "    return bias, weights, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, loss = gradient_descent_multi_variable(X, y, lr = 1e-5, number_of_iterations = 250)\n",
    "loss_initial = loss[0]\n",
    "loss_final = loss[-1]\n",
    "\n",
    "assert loss_initial > loss_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: Which algorithm should we use for Linear Regression?\n",
    "\n",
    "Compare the pros and cons of using normal equation and gradient descent for linear\n",
    "regression. Moreover, select the algorithm you think is more suitable for this problem\n",
    "set and explain why you choose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Polynomial Regression\n",
    "\n",
    "In real-world data, a straight line might not fit the data perfectly. Consider the relation between **schools** and **asking_price**.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"imgs/school_price_rel.png\" alt=\"school price relation\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 5: Schools - Price Relationship.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Houses with 0 schools nearby tend to be cheaper than houses with 1 school nearby. However, as the number of schools increases, the prices decrease. A polynomial function can better capture this relationship. A polynomial function is written as follows:\n",
    "\n",
    "$$ y = w_0 + w_1 x + w_2 x^2 + ... + w_n x^n $$\n",
    "\n",
    "where $y$ is the target value, $x$ is a (*single*) feature value, and $n$ is the degree of the polynomial. $w_0$ and $w_1, \\dots, w_n$ are as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 : Create Polynomial Matrix\n",
    "\n",
    "Write the function `create_polynomial_matrix(X, power)` that takes a $(m, 1)$-matrix and an\n",
    "integer, and returns a polynomial matrix with shape $(m, power)$.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"imgs/poly_matrix.png\" alt=\"polynomial matrix\" width=\"50%\">\n",
    "<figcaption style=\"text-align:center\">Figure 6: Example of creating a polynomial matrix.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polynomial_matrix(X, power = 2):\n",
    "    '''\n",
    "    Create a polynomial matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: (m, 1) numpy matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (m, power) numpy matrix where the i-th column denotes\n",
    "            X raised to the power of i.\n",
    "    '''\n",
    "    # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "    X = np.tile(X, (1,power))\n",
    "    X = np.cumprod(X, axis = 1)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([[1], [2], [3]])\n",
    "poly_matrix = np.array([[1, 1, 1], [2, 4, 8], [3, 9, 27]])\n",
    "\n",
    "assert np.array_equal(create_polynomial_matrix(vector, 3), poly_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Get the prediction line\n",
    "\n",
    "Write the function `get_prediction_poly_regression(X, y, power, include_bias)` that returns\n",
    "`y_pred`, a vector of predicted values for the training data.\n",
    "\n",
    "**Note**: You can use functions from before for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_poly_regression(X, y, power = 2, include_bias = True):\n",
    "    '''\n",
    "    Calculate the best polynomial line.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, 1) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    power (int) : Specify the degree of the polynomial\n",
    "    include_bias (boolean) : Specify whether the model should include a bias term\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (m, 1) numpy matrix representing prediction values.\n",
    "    '''\n",
    "    X = create_polynomial_matrix(X, 2)\n",
    "    y_pred = get_prediction_linear_regression(X, y, include_bias)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [5.32907052e-15]]\n"
     ]
    }
   ],
   "source": [
    "test_X, test_y = np.arange(3).reshape((-1, 1)), np.arange(4, 7).reshape((-1, 1))\n",
    "pred_y = get_prediction_poly_regression(test_X, test_y, 2)\n",
    "\n",
    "assert round(mean_squared_error(test_y, pred_y), 5) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check your work, create a polynomial regression model, using `power = 3` and `include_bias = True`, with **schools** as the only feature and **asking_price** as the target value. Plot your prediction line using the code snippet below. It should look similar to the Figure 7.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"imgs/poly_reg.png\" alt=\"polynomial regression\" width=\"50%\">\n",
    "    <figcaption style=\"text-align:center\">Figure 7: Example of polynomial regression using <b>schools</b> as feature.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[623622.71719835]\n",
      " [-22380.69864306]\n",
      " [ -1100.46767831]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "schools = X[:, 2].reshape((-1, 1))\n",
    "predicted = get_prediction_poly_regression(schools, y, 3)\n",
    "plt.scatter(schools, y)\n",
    "plt.scatter(schools, predicted, color = 'r', s = 100)\n",
    "plt.xlabel(\"Number of schools within 1km\")\n",
    "plt.ylabel(\"Price in SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.3: Feature Scaling\n",
    "\n",
    "As we create a higher degree polynomial matrix, each column will have a larger scale\n",
    "than the previous one. This can lead to poor performance for gradient descent. Here\n",
    "is where feature scaling plays an important role. Write the function `feature_scaling(X)`\n",
    "that takes a NumPy matrix `X` and returns a mean-normalized matrix.\n",
    "\n",
    "**Note**: The normalization occurs on the column level (i-th column is normalized by the\n",
    "mean and standard deviation of the i-th column). That is,\n",
    "\n",
    "$$\n",
    "\\text{If} \\quad \\boldsymbol{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_k \\end{pmatrix} \\\\\n",
    "\\boldsymbol{v}_{norm} = \\frac{\\boldsymbol{v} - \\boldsymbol{\\hat{v}}}{\\sigma_{v}}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{v}$ is a vector of $k$ elements, $\\boldsymbol{\\hat{v}}$ is its mean and $\\sigma_{v}$ is its standard deviation.\n",
    "\n",
    "&nbsp;\n",
    "<figure>\n",
    "<img src=\"imgs/feature_scaling.png\" alt=\"feature scaling\" width=\"50%\">\n",
    "    <figcaption style=\"text-align:center\">Figure 8: Example of a matrix before and after feature scaling.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_scaling(X):\n",
    "    '''\n",
    "    Mean normalized each feature column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        A (m, n) numpy matrix where each column has been mean-normalized.\n",
    "    '''\n",
    "    # TODO: add your solution here and remove `raise NotImplementedError`\n",
    "    mean = np.mean(X, axis = 0)\n",
    "    std = np.std(X, axis = 0)\n",
    "    return (X - mean) / std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_X = np.array([[1, 2], [3, 4]])\n",
    "expected = np.array([[-1, -1], [1, 1]])\n",
    "\n",
    "assert np.array_equal(feature_scaling(public_X), expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.4: Find number of iterations to converge\n",
    "\n",
    "Fill in the function `find_number_of_iterations(X, y, lr, delta_loss)` that that returns:\n",
    "\n",
    "* $w_0$ - a number representing the bias constant\n",
    "* $w_1, w_2, \\dots, w_n$ - $(n, 1)$ NumPy matrix, where each element denotes the weight constant of a certain feature\n",
    "* $num\\_of\\_iterations$ - a number representing the number of iterations performed to reach convergence\n",
    "\n",
    "The definition of convergence is as follows:\n",
    "\n",
    "$$ |J_{t-1} - J_{t}| < delta\\_loss $$\n",
    "\n",
    "where $J_{t-1}$ is loss at timestep $t-1$ (previous timestep), $J_{t}$ is loss at timestep $t$ (current timestep), and $delta\\_loss$ is the termination criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_of_iterations(X, y, lr, delta_loss):\n",
    "    '''\n",
    "    Do gradient descent until convergence and return number of iterations\n",
    "    required.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X (np.ndarray) : (m, n) numpy matrix representing feature matrix\n",
    "    y (np.ndarray) : (m, 1) numpy matrix representing target values\n",
    "    lr (float) : Learning rate\n",
    "    delta_loss (float) : Termination criterion\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        bias (float):\n",
    "            The bias constant\n",
    "        weights (np.ndarray):\n",
    "            A (n, 1) numpy matrix that specifies the weight constants.\n",
    "        num_of_iterations (int):\n",
    "            Number of iterations to reach convergence\n",
    "    '''\n",
    "    # Do not change\n",
    "    bias = 0\n",
    "    weights = np.full((X.shape[1], 1), 0).astype(float)\n",
    "    num_of_iterations = 0\n",
    "    previous_loss = 1e14\n",
    "    current_loss = -1e14\n",
    "\n",
    "    m, n = np.shape(X)\n",
    "\n",
    "\n",
    "    while abs(previous_loss - current_loss) >= delta_loss:\n",
    "        y_pred = X @ weights + bias\n",
    "        previous_loss = mean_squared_error(y, y_pred)\n",
    "\n",
    "        grad_bias = np.sum(y_pred - y) / m\n",
    "        grad_weights = np.sum(((y_pred - y) * X), axis = 0) / m\n",
    "\n",
    "        temp_bias = bias - lr * grad_bias\n",
    "        temp_weights = weights - lr * grad_weights\n",
    "        \n",
    "        bias = temp_bias\n",
    "        weights = temp_weights\n",
    "\n",
    "        y_pred = X @ weights + bias\n",
    "        current_loss = mean_squared_error(y, y_pred)\n",
    "        num_of_iterations += 1\n",
    "    \n",
    "    return bias, weights, num_of_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_X = create_polynomial_matrix(X[:, 2].reshape((-1, 1)), 3)\n",
    "_, _, num_of_iterations = find_number_of_iterations(poly_X, y, 1e-5, 1e7)\n",
    "\n",
    "assert num_of_iterations > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.5: Analyze the effects of feature scaling on Gradient Descent\n",
    "\n",
    "For this task, take the **schools** column and transform it into a 3rd-degree polynomial\n",
    "matrix. We will use this as our feature matrix.\n",
    "\n",
    "Select at least **3 different learning rates** and tabulate number of iterations taken to\n",
    "reach convergence for each learning rate using the `find_number_of_iterations` function in\n",
    "Task 3.4. Do the same on the mean-normalized polynomial matrix. Compare the number of\n",
    "iterations taken between normalized and non-normalized polynomial matrices.\n",
    "\n",
    "What can you conclude from this observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non normalized num_iterations:\n",
      "1946 90 57\n",
      "normalized num_iterations:\n",
      "1193 23 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/py/_k054zjd6zsf783gqr010ktw0000gn/T/ipykernel_49979/2349018527.py:15: RuntimeWarning: overflow encountered in square\n",
      "  result = np.sum(np.square(y_pred - y_target))\n",
      "/Users/bryanongjx/opt/anaconda3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "/var/folders/py/_k054zjd6zsf783gqr010ktw0000gn/T/ipykernel_49979/1137469138.py:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  while abs(previous_loss - current_loss) >= delta_loss:\n"
     ]
    }
   ],
   "source": [
    "X = create_polynomial_matrix(schools)\n",
    "\n",
    "# not normalized\n",
    "x1, x2, x3 = find_number_of_iterations(X, y, 0.001, 1e7)[2], find_number_of_iterations(X, y, 0.1, 1e7)[2], find_number_of_iterations(X, y, 1, 1e7)[2]\n",
    "\n",
    "print('non normalized num_iterations:')\n",
    "print(x1, x2, x3)\n",
    "\n",
    "# normalized \n",
    "X_norm = feature_scaling(X)\n",
    "x1, x2, x3 = find_number_of_iterations(X_norm, y, 0.001, 1e7)[2], find_number_of_iterations(X_norm, y, 0.1, 1e7)[2], find_number_of_iterations(X_norm, y, 1, 1e7)[2]\n",
    "print('normalized num_iterations:')\n",
    "print(x1, x2, x3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Once you are done, please submit your work to Coursemology, by copying the right\n",
    "snippets of code into the corresponding box that says “Your answer,” and click “Save.”\n",
    "After you save, you can still make changes to your submission.\n",
    "\n",
    "Once you are satisfied with what you have uploaded, click “Finalize submission.” Note\n",
    "that once your submission is finalized, it is considered to be submitted for grading\n",
    "and cannot be changed. If you need to undo this action, you will have to email your\n",
    "assigned tutor for help. Please do not finalize your submission until you are sure that\n",
    "you want to submit your solutions for grading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b7352a20b64d7ded3c1224754ce01461a26994ddd5a0a1dd2a26e94425d0c7c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
